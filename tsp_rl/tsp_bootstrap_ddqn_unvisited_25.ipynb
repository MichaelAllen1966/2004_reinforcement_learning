{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from env.tsp_env import TspEnv\n",
    "from utils import tsp_plots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CITIES = 25\n",
    "NUMBER_OF_NETS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount rate of future rewards\n",
    "GAMMA = 1.0\n",
    "# Learing rate for neural network\n",
    "LEARNING_RATE = 0.001\n",
    "# Maximum number of game steps (state, action, reward, next state) to keep\n",
    "MEMORY_SIZE = 100000\n",
    "# Frequency of neural net \n",
    "BATCH_SIZE = 5\n",
    "# Number of game steps to play before starting training\n",
    "REPLAY_START_SIZE = 1000\n",
    "# Exploration rate (episolon) is probability of choosing a random action\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "# Reduction in epsilon with each game step\n",
    "EXPLORATION_DECAY = 0\n",
    "# Number of steps between target network update\n",
    "SYNC_TARGET_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stopping conditions\n",
    "MAXIMUM_RUNS = 50000\n",
    "MAXIMUM_TIME_MINS =  480\n",
    "NO_IMPROVEMENT_RUNS = 5000\n",
    "NO_IMPROVEMENT_TIME = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set whether to plot all new best routes as they are found\n",
    "PLOT_NEW_BEST_ROUTES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Deep Q Network solver. Includes control variables, memory of \n",
    "    state/action/reward/end, neural net,and methods to act, \n",
    "    remember, and update neural net by sampling from memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, n_actions):\n",
    "        \"\"\"Constructor method. Set up memory and neural nets.\"\"\"\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Set starting exploration rate\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        \n",
    "        # Set up memory for state/action/reward/next_state/done\n",
    "        # Deque will drop old data when full\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        # Set up neural net\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_space, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "            )\n",
    "        \n",
    "        # Set loss function and optimizer\n",
    "        self.objective = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Act either randomly or by redicting action that gives max Q\"\"\"\n",
    "\n",
    "        # Set up a dataframe array of q, city index, random\n",
    "        action_df = pd.DataFrame()\n",
    "        action_df['city'] = np.arange(NUMBER_OF_CITIES)\n",
    "        q_values = self.net(torch.FloatTensor(state))\n",
    "        action_df['q'] = q_values = q_values.detach().numpy()[0]\n",
    "        action_df['visted'] = state[0][0:len(q_values)]\n",
    "        action_df['random'] = np.random.random(NUMBER_OF_CITIES)\n",
    "        \n",
    "        # Filter to unvisted cities\n",
    "        mask = action_df['visted'] == 0\n",
    "        action_df = action_df[mask]\n",
    "        \n",
    "        # If no unvisted cities, action is to retuen to city 0\n",
    "        if mask.sum() == 0:\n",
    "            action = 0\n",
    "        else:\n",
    "            # Sort action table by Q or random order (for exploration)\n",
    "            sort_col = ('random' if np.random.rand() < self.exploration_rate\n",
    "                        else 'q')\n",
    "            action_df.sort_values(sort_col, ascending=False, inplace=True)\n",
    "            action = int(action_df['city'].iloc[0])\n",
    "\n",
    "        return  action\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed forward function for neural net\"\"\"\n",
    "        \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \"\"\"\n",
    "    Replay memory used to train model.\n",
    "    Limited length memory (using deque, double ended queue from collections).\n",
    "    Holds, state, action, reward, next state, and episode done.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor method to initialise replay memory\"\"\"\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"state/action/reward/next_state/done\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(policy_net, target_net, memory):\n",
    "    \"\"\"\n",
    "    Update  model by sampling from memory.\n",
    "    Uses policy network to predict best action (best Q).\n",
    "    Uses target network to provide target of Q for the selected next action.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Do not try to train model if memory is less than reqired batch size\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return    \n",
    " \n",
    "    # Reduce exploration rate\n",
    "    policy_net.exploration_rate *= EXPLORATION_DECAY\n",
    "    policy_net.exploration_rate = max(EXPLORATION_MIN, \n",
    "                                      policy_net.exploration_rate)\n",
    "    # Sample a random batch from memory\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    for state, action, reward, state_next, terminal in batch:\n",
    "        \n",
    "        state_action_values = policy_net(torch.FloatTensor(state))\n",
    "       \n",
    "        if not terminal:\n",
    "            # For non-terminal actions get Q from policy net\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach next state values from gradients to prevent updates\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Get next state action with best Q from the policy net (double DQN)\n",
    "            policy_next_state_values = policy_net(torch.FloatTensor(state_next))\n",
    "            policy_next_state_values = policy_next_state_values.detach()\n",
    "            best_action = np.argmax(policy_next_state_values[0].numpy())\n",
    "            # Get targen net next state\n",
    "            next_state_action_values = target_net(torch.FloatTensor(state_next))\n",
    "            # Use detach again to prevent target net gradients being updated\n",
    "            next_state_action_values = next_state_action_values.detach()\n",
    "            best_next_q = next_state_action_values[0][best_action].numpy()\n",
    "            updated_q = reward + (GAMMA * best_next_q)      \n",
    "            expected_state_action_values[0][action] = updated_q\n",
    "        else:\n",
    "            # For termal actions Q = reward (-1)\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach values from gradients to prevent gradient update\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Set Q for all actions to reward (-1)\n",
    "            expected_state_action_values[0] = reward\n",
    "\n",
    "        # Update neural net\n",
    "        \n",
    "        # Reset net gradients\n",
    "        policy_net.optimizer.zero_grad()  \n",
    "        # calculate loss\n",
    "        loss_v = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "        # Backpropogate loss\n",
    "        loss_v.backward()\n",
    "        # Update network gradients\n",
    "        policy_net.optimizer.step()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main program loop\"\"\"\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Set up environment\n",
    "    time_start = time.time()\n",
    "    env = TspEnv(number_of_cities = NUMBER_OF_CITIES, \n",
    "                 grid_dimensions = (100,100))\n",
    "\n",
    "    \n",
    "    # Get number of observations returned for state\n",
    "    observation_space = env.observation_space.shape[0] * 2\n",
    "    \n",
    "    # Get number of actions possible\n",
    "    n_actions = len(env.action_space)\n",
    "    \n",
    "    # Set up policy and target neural nets\n",
    "    policy_nets = [DQN(observation_space, n_actions) for i in range(NUMBER_OF_NETS)]\n",
    "    target_nets = [DQN(observation_space, n_actions) for i in range(NUMBER_OF_NETS)]\n",
    "    \n",
    "    # Copy weights from policy_net to target\n",
    "    for i in range(NUMBER_OF_NETS):\n",
    "        target_nets[i].load_state_dict(policy_nets[i].state_dict())\n",
    "        # Set target net to eval rather than training mode\n",
    "        # We do not train target net - ot is copied from policy net at intervals\n",
    "        target_nets[i].eval() \n",
    "        \n",
    "    # Set up a single memorymemomry\n",
    "    memory = Memory()\n",
    "    \n",
    "    # Set up list for results\n",
    "    results_run = []\n",
    "    results_exploration = []\n",
    "    total_rewards = []\n",
    "    best_reward = -999999\n",
    "    best_route = None\n",
    "    \n",
    "    # Set run and time of last best route\n",
    "    run_last_best = 0\n",
    "    time_last_best = time.time()\n",
    "\n",
    "    # Set up run counter and learning loop\n",
    "    step = 0\n",
    "    run = 0\n",
    "    continue_learning = True\n",
    "    \n",
    "    # Continue repeating games (episodes) until target complete\n",
    "    while continue_learning:\n",
    "        \n",
    "        # Increment run (episode) counter\n",
    "        run += 1\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Start run and get first state observations\n",
    "        state, reward, terminal, info = env.reset()\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Reshape state into 2D array with state obsverations as first 'row'\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        \n",
    "        # Reset route\n",
    "        route = []\n",
    "        \n",
    "        # Episode loop\n",
    "        while True:\n",
    "            \n",
    "            # Increment step counter\n",
    "            step += 1\n",
    "            \n",
    "            actions = [policy_nets[i].act(state) for i in range(NUMBER_OF_NETS)]\n",
    "            random_index = random.randint(0, NUMBER_OF_NETS - 1)\n",
    "            action = actions[random_index] \n",
    "            \n",
    "            route.append(action)\n",
    "            \n",
    "            # Act\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            total_reward += reward\n",
    "                     \n",
    "            # Get observations for new state (s')\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "                        \n",
    "            # Record state, action, reward, new state & terminal\n",
    "            memory.remember(state, action, reward, state_next, terminal)\n",
    "            \n",
    "            # Update state\n",
    "            state = state_next\n",
    "            \n",
    "            # Update neural net            \n",
    "            \n",
    "            if len(memory.memory) >= REPLAY_START_SIZE:\n",
    "                # Update policy net \n",
    "                for i in range(NUMBER_OF_NETS):\n",
    "                    optimize(policy_nets[i], target_nets[i], memory.memory)\n",
    "                \n",
    "                # Update the target network at intervals\n",
    "                if step % SYNC_TARGET_STEPS == 0:\n",
    "                    for i in range(NUMBER_OF_NETS):\n",
    "                        target_nets[i].load_state_dict(policy_nets[i].state_dict())\n",
    "        \n",
    "                 \n",
    "            # Actions to take if end of game episode\n",
    "            if terminal:\n",
    "                # Clear print row content\n",
    "                clear_row = '\\r' + ' '*100 + '\\r'\n",
    "                print (clear_row, end ='')\n",
    "                print (f'Run: {run: 5.0f}, ', end='')\n",
    "                exp = policy_nets[0].exploration_rate\n",
    "                print (f'exploration: {exp: 4.3f}, ', end='')\n",
    "                print (f'total reward: {total_reward: 6.0f}', end='')\n",
    "                \n",
    "                # Add to results lists\n",
    "                results_run.append(run)\n",
    "                results_exploration.append(policy_nets[0].exploration_rate)\n",
    "                total_rewards.append(total_reward)\n",
    "                \n",
    "                # Check for best route so far\n",
    "                if total_reward > best_reward:\n",
    "                    best_reward = total_reward\n",
    "                    best_route = route\n",
    "                    run_last_best = run\n",
    "                    time_last_best = time.time()\n",
    "                    time_elapsed = (time.time() - time_start) / 60\n",
    "                    print(f'\\nNew best run. Run : {run: 5.0f},  ' \\\n",
    "                          f'Time {time_elapsed: 4.0f}  ' \\\n",
    "                          f'Reward {total_reward: 6.0f}')\n",
    "                    # Plot new best route\n",
    "                    if PLOT_NEW_BEST_ROUTES and step > REPLAY_START_SIZE:\n",
    "                        if best_reward > 0:\n",
    "                            tsp_plots.plot_route(env, best_route)\n",
    "                            print()\n",
    " \n",
    "                                \n",
    "                # Check stopping conditions\n",
    "                stop = False\n",
    "                if step > REPLAY_START_SIZE:\n",
    "                    if run == MAXIMUM_RUNS:\n",
    "                        stop = True\n",
    "                    elif time.time() - time_start > MAXIMUM_TIME_MINS * 60:\n",
    "                        stop = True\n",
    "                    elif time.time() - time_last_best > NO_IMPROVEMENT_TIME*60:\n",
    "                        stop = True\n",
    "                    elif run - run_last_best == NO_IMPROVEMENT_RUNS:\n",
    "                        stop = True\n",
    "                        \n",
    "                if stop:\n",
    "                    # End training\n",
    "                    continue_learning = False\n",
    "     \n",
    "                # End episode\n",
    "                break\n",
    "            \n",
    "    ############################# Plot results #################################\n",
    "        \n",
    "    # Plot result progress\n",
    "    tsp_plots.plot_result_progress(total_rewards)\n",
    "    \n",
    "    # Plot best route\n",
    "    tsp_plots.plot_route(env, best_route)\n",
    "    \n",
    "    ###################### Show route and distances ############################\n",
    "    \n",
    "    print ('Route')\n",
    "    print (best_route)\n",
    "    print ()\n",
    "    print ('Best route distance')\n",
    "    print (f'{env.state.calculate_distance(best_route):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
