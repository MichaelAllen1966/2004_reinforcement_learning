{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travelling Salesman Problem Reinforcement Learning - Cross Entropy\n",
    "\n",
    "The travelling salesman problem is a classic optimisation problem, where the aim is to find the fastest or shortest route between a numebr of cities, returning to the base city at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL involves:\n",
    "* Trial and error search\n",
    "* Receiving and maximising reward (often delayed)\n",
    "* Linking state -> action -> reward\n",
    "* Must be able to sense something of their environment\n",
    "* Involves uncertainty in sensing and linking action to reward\n",
    "* Learning -> improved choice of actions over time\n",
    "* All models find a way to balance best predicted action vs. exploration\n",
    "\n",
    "### Elements of RL\n",
    "* *Environment*: all observable and unobservable information relevant to us\n",
    "* *Observation*: sensing the environment\n",
    "* *State*: the perceived (or perceivable) environment \n",
    "* *Agent*: senses environment, decides on action, receives and monitors rewards\n",
    "* *Action*: may be discrete (e.g. turn left) or continuous (accelerator pedal)\n",
    "* *Policy* (how to link state to action; often based on probabilities)\n",
    "* *Reward signal*: aim is to accumulate maximum reward over time\n",
    "* *Value function* of a state: prediction of likely/possible long-term reward from a particular state\n",
    "* *Q*: prediction of likely/possible long-term reward from performing a particular *action*\n",
    "* *Model* (optional): a simulation of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Cross entropy is a policy method (it seeks to recommend actions without any estimation of the value of a state or action). The basic method is:\n",
    "\n",
    "1. Play *k* episodes using current model and environment\n",
    "2. Calculate total reward for each episode\n",
    "3. Keep best episodes (e.g. top 50%)\n",
    "4. Train on remaining episodes using observations and actions taken\n",
    "5. Repeat\n",
    "\n",
    "![](./images/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Import required liraries (including environment and plotting libraries in `env` folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from env.tsp_env import TspEnv\n",
    "from utils import tsp_plots\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set number of cities to visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CITIES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set stopping conditions\n",
    "\n",
    "Stop if any of the following are true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXIMUM_BATCHES = 5000\n",
    "MAXIMUM_TIME_MINS = 120\n",
    "NO_IMPROVEMENT_BATCHES = 250\n",
    "NO_IMPROVEMENT_TIME = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set number of episodes to run each generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch neural net using the flexible pytorch nn.Modlule class.\n",
    "    Note: the neural net output is linear. To convert these to probabilities for\n",
    "    each action (sum to 1.0) a SoftMax activation on the final output is\n",
    "    required, but this is applied outside of the net itself, which improves\n",
    "    speed and stability of training.\n",
    "\n",
    "    Layers in model:\n",
    "    * Input layer (implied, takes the number of observations)\n",
    "    * Densely connected layer (size = 4 x number of observations)\n",
    "    * ReLU activation\n",
    "    * Densely connected layer (size = 4 x number of observations)\n",
    "    * ReLU activation\n",
    "    * Output layer (size = number of possible actions)\n",
    "\n",
    "    (A SoftMax layer will be added later)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        \"\"\"Define layers of sequential net\"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_space, NUMBER_OF_CITIES * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUMBER_OF_CITIES * 4, NUMBER_OF_CITIES * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUMBER_OF_CITIES * 4, NUMBER_OF_CITIES * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUMBER_OF_CITIES * 4, action_space)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Define forward pass (simple, as using a pre-defined sequential\n",
    "        model)\"\"\"\n",
    "        \n",
    "        # Move input to required device (GPU or CPU)\n",
    "        # Pass through net\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, model, observation_space):\n",
    "    \"\"\"Play an episode\"\"\"\n",
    "\n",
    "    # Define softmax layer\n",
    "    sm = nn.Softmax(dim=1)\n",
    "\n",
    "    # Reset trackers and environment\n",
    "    episode_reward = 0\n",
    "    obs_tracker = []\n",
    "    action_tracker = []\n",
    "\n",
    "    # Reset environment (returns first observation)\n",
    "    obs, reward, is_terminal, info = env.reset()\n",
    "\n",
    "    # Loop up to 200 steps\n",
    "    for step in range(500):\n",
    "\n",
    "        # Track observations\n",
    "        obs = np.float32(obs)\n",
    "        obs_tracker.append(obs)\n",
    "\n",
    "        # Get action probability (put obs in Tensor first)\n",
    "        obs = torch.FloatTensor([obs])\n",
    "        act_probs = model(obs)\n",
    "        act_probs = sm(act_probs)\n",
    "        act_probs = act_probs.data.numpy()[0]\n",
    "\n",
    "        # Get and track action: action sampled based on probability distribution\n",
    "        np.random.seed()\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        action_tracker.append(action)\n",
    "\n",
    "        # Take action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Pole has fallen over if done is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Put results in dictionary\n",
    "    results = {'episode_reward': episode_reward,\n",
    "               'episode_obs': obs_tracker,\n",
    "               'episode_actions': action_tracker}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_episodes(results, percentile_cutoff=70):\n",
    "    \"\"\"Get best episodes\"\"\"\n",
    "\n",
    "    # Get episode rewards & define cuttoff\n",
    "    episode_rewards = [episode['episode_reward'] for episode in results]\n",
    "    reward_cutoff = np.percentile(episode_rewards, percentile_cutoff)\n",
    "    \n",
    "    # Get best route\n",
    "    best_route_index = np.argmax(episode_rewards)\n",
    "    \n",
    "    all_actions = actions = [episode['episode_actions'] for episode in results]\n",
    "    best_route = all_actions[best_route_index]\n",
    "\n",
    "    # Get best episode observations and actions\n",
    "    obs = [episode['episode_obs'] for episode in results if\n",
    "           episode['episode_reward'] >= reward_cutoff]\n",
    "\n",
    "    actions = [episode['episode_actions'] for episode in results if\n",
    "               episode['episode_reward'] >= reward_cutoff]\n",
    "\n",
    "    # Convert list of observation arrays into a numpy array\n",
    "    obs = np.vstack(obs)\n",
    "\n",
    "    # Flatten actions list and convert to NumPy\n",
    "    actions = [item for sublist in actions for item in sublist]\n",
    "    actions = np.array(actions)\n",
    "    \n",
    "    return obs, actions, best_route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ############################## Set up model ################################\n",
    "\n",
    "    # Set up environment\n",
    "    time_start = time.time()\n",
    "    env = TspEnv(number_of_cities = NUMBER_OF_CITIES)\n",
    "\n",
    "    # Get number of observations from environemt(allows the env to change)\n",
    "    # Obs = array of visited cities and on-ehot array of current city\n",
    "    obs_size = env.observation_space.shape[0] * 2\n",
    "\n",
    "    # Get number of actins from environemnt\n",
    "    n_actions = len(env.action_space)\n",
    "\n",
    "    # Set up Neural Net\n",
    "    model = Net(obs_size, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.003\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Play batches of episodes\n",
    "    batch_count = 0\n",
    "\n",
    "    # Lists to store results\n",
    "    all_results_batch_number = []\n",
    "    all_results_average_reward = []\n",
    "    all_results_maxumum_reward = []\n",
    "    last_results = deque(maxlen = 10)\n",
    "    \n",
    "    # Track best\n",
    "    best_reward = -9999999\n",
    "    best_average_reward = -9999999\n",
    "    best_route = None\n",
    "    \n",
    "    # Set batch and time of last best route\n",
    "    batch_last_best = 0\n",
    "    time_last_best = time.time()\n",
    "\n",
    "    ################## Play batch of epsiodes and select best ##################\n",
    "    \n",
    "    # Start playing loop and continue until goal performance reached\n",
    "    while True:\n",
    "        # Play episodes\n",
    "        batch_count += 1\n",
    "        batch_results = []\n",
    "        for episode in range(EPISODE_BATCH_SIZE):\n",
    "            results = play_episode(env, model, obs_size)\n",
    "            batch_results.append(results)\n",
    "\n",
    "        # Get average and maximum reward\n",
    "        rewards = [episode['episode_reward'] for episode in batch_results]\n",
    "        average_reward = np.mean(rewards)\n",
    "        maximum_reward = np.max(rewards)\n",
    "        \n",
    "        # Get best runs\n",
    "        training_obs, training_actions, batch_best_route = \\\n",
    "            filter_episodes(batch_results)\n",
    "        training_obs = torch.Tensor(training_obs)\n",
    "        training_actions = torch.Tensor(training_actions).long()        \n",
    "   \n",
    "        # Check whether best route is better than previously discovered\n",
    "        if maximum_reward > best_reward:\n",
    "            best_reward = maximum_reward\n",
    "            best_route = batch_best_route\n",
    "        \n",
    "        if average_reward > best_average_reward:\n",
    "            best_average_reward = average_reward\n",
    "            batch_last_best = batch_count\n",
    "            time_last_best = time.time()\n",
    "            time_elapsed = (time.time() - time_start) / 60\n",
    "            print(f'\\nNew best average. Batch : {batch_count: 5.0f},  ' \\\n",
    "                  f'Time {time_elapsed: 4.0f}  ' \\\n",
    "                  f'Average reward {average_reward: 6.0f}')\n",
    "\n",
    "        # Store and print results\n",
    "        all_results_batch_number.append(batch_count)\n",
    "        all_results_average_reward.append(average_reward)\n",
    "        all_results_maxumum_reward.append(maximum_reward)\n",
    "        last_results.append(average_reward)\n",
    "\n",
    "        print(f'\\rBatch {batch_count:3}. Average and best run: ' \\\n",
    "              f'{average_reward:6.0f}, {maximum_reward:6.0f}. ', end=\"\")\n",
    "        \n",
    "        # Check for stopping\n",
    "        stop = False\n",
    "        if batch_count == MAXIMUM_BATCHES:\n",
    "            stop = True\n",
    "        elif time.time() - time_start > MAXIMUM_TIME_MINS * 60:\n",
    "            stop = True\n",
    "        elif time.time() - time_last_best > NO_IMPROVEMENT_TIME * 60:\n",
    "            stop = True\n",
    "        elif batch_count - batch_last_best > NO_IMPROVEMENT_BATCHES:\n",
    "            stop = True\n",
    "        elif average_reward == maximum_reward:\n",
    "            # Model converged so all runs identical\n",
    "            stop = True\n",
    "  \n",
    "        if stop:\n",
    "            break \n",
    "\n",
    "        \n",
    "        ############### Train model on best episodes from batch ################\n",
    "       \n",
    "        # Reset model gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Predict actions\n",
    "        action_scores = model(training_obs)\n",
    "        # Calculate loss between predicted and actual actions\n",
    "        loss_v = objective(action_scores, training_actions)\n",
    "        # Back propgate losses\n",
    "        loss_v.backward()\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    ############################# Plot results #################################\n",
    "        \n",
    "    # Plot result progress\n",
    "    tsp_plots.plot_result_progress_cross_entropy(\n",
    "        all_results_batch_number, \n",
    "        all_results_average_reward, \n",
    "        all_results_maxumum_reward)\n",
    "    \n",
    "    # Plot best route\n",
    "    tsp_plots.plot_route(env, best_route)\n",
    "    \n",
    "    ###################### Show route and distances ############################\n",
    "    \n",
    "    print ('Route')\n",
    "    print (best_route)\n",
    "    print ()\n",
    "    print ('Best route distance')\n",
    "    print (f'{env.state.calculate_distance(best_route):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ############################## Set up model ################################\n",
    "\n",
    "    # Set up environment\n",
    "    time_start = time.time()\n",
    "    env = TspEnv(number_of_cities = NUMBER_OF_CITIES)\n",
    "\n",
    "    # Get number of observations from environemt(allows the env to change)\n",
    "    # Obs = array of visited cities and on-ehot array of current city\n",
    "    obs_size = env.observation_space.shape[0] * 2\n",
    "\n",
    "    # Get number of actins from environemnt\n",
    "    n_actions = len(env.action_space)\n",
    "\n",
    "    # Set up Neural Net\n",
    "    model = Net(obs_size, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.003\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Play batches of episodes\n",
    "    batch_count = 0\n",
    "\n",
    "    # Lists to store results\n",
    "    all_results_batch_number = []\n",
    "    all_results_average_reward = []\n",
    "    all_results_maxumum_reward = []\n",
    "    last_results = deque(maxlen = 10)\n",
    "    \n",
    "    # Track best\n",
    "    best_reward = -9999999\n",
    "    best_average_reward = -9999999\n",
    "    best_route = None\n",
    "    \n",
    "    # Set batch and time of last best route\n",
    "    batch_last_best = 0\n",
    "    time_last_best = time.time()\n",
    "\n",
    "    ################## Play batch of epsiodes and select best ##################\n",
    "    \n",
    "    # Start playing loop and continue until goal performance reached\n",
    "    while True:\n",
    "        # Play episodes\n",
    "        batch_count += 1\n",
    "        batch_results = []\n",
    "        for episode in range(EPISODE_BATCH_SIZE):\n",
    "            results = play_episode(env, model, obs_size)\n",
    "            batch_results.append(results)\n",
    "\n",
    "        # Get average and maximum reward\n",
    "        rewards = [episode['episode_reward'] for episode in batch_results]\n",
    "        average_reward = np.mean(rewards)\n",
    "        maximum_reward = np.max(rewards)\n",
    "        \n",
    "        # Get best runs\n",
    "        training_obs, training_actions, batch_best_route = \\\n",
    "            filter_episodes(batch_results)\n",
    "        training_obs = torch.Tensor(training_obs)\n",
    "        training_actions = torch.Tensor(training_actions).long()        \n",
    "   \n",
    "        # Check whether best route is better than previously discovered\n",
    "        if maximum_reward > best_reward:\n",
    "            best_reward = maximum_reward\n",
    "            best_route = batch_best_route\n",
    "        \n",
    "        if average_reward > best_average_reward:\n",
    "            best_average_reward = average_reward\n",
    "            batch_last_best = batch_count\n",
    "            time_last_best = time.time()\n",
    "            time_elapsed = (time.time() - time_start) / 60\n",
    "            print(f'\\nNew best average. Batch : {batch_count: 5.0f},  ' \\\n",
    "                  f'Time {time_elapsed: 4.0f}  ' \\\n",
    "                  f'Average reward {average_reward: 6.0f}')\n",
    "\n",
    "        # Store and print results\n",
    "        all_results_batch_number.append(batch_count)\n",
    "        all_results_average_reward.append(average_reward)\n",
    "        all_results_maxumum_reward.append(maximum_reward)\n",
    "        last_results.append(average_reward)\n",
    "\n",
    "        print(f'\\rBatch {batch_count:3}. Average and best run: ' \\\n",
    "              f'{average_reward:6.0f}, {maximum_reward:6.0f}. ', end=\"\")\n",
    "        \n",
    "        # Check for stopping\n",
    "        stop = False\n",
    "        if batch_count == MAXIMUM_BATCHES:\n",
    "            stop = True\n",
    "        elif time.time() - time_start > MAXIMUM_TIME_MINS * 60:\n",
    "            stop = True\n",
    "        elif time.time() - time_last_best > NO_IMPROVEMENT_TIME * 60:\n",
    "            stop = True\n",
    "        elif batch_count - batch_last_best > NO_IMPROVEMENT_BATCHES:\n",
    "            stop = True\n",
    "        elif average_reward == maximum_reward:\n",
    "            # Model converged so all runs identical\n",
    "            stop = True\n",
    "  \n",
    "        if stop:\n",
    "            break \n",
    "\n",
    "        \n",
    "        ############### Train model on best episodes from batch ################\n",
    "       \n",
    "        # Reset model gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Predict actions\n",
    "        action_scores = model(training_obs)\n",
    "        # Calculate loss between predicted and actual actions\n",
    "        loss_v = objective(action_scores, training_actions)\n",
    "        # Back propgate losses\n",
    "        loss_v.backward()\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    ############################# Plot results #################################\n",
    "        \n",
    "    # Plot result progress\n",
    "    tsp_plots.plot_result_progress_cross_entropy(\n",
    "        all_results_batch_number, \n",
    "        all_results_average_reward, \n",
    "        all_results_maxumum_reward)\n",
    "    \n",
    "    # Plot best route\n",
    "    tsp_plots.plot_route(env, best_route)\n",
    "    \n",
    "    ###################### Show route and distances ############################\n",
    "    \n",
    "    print ('Route')\n",
    "    print (best_route)\n",
    "    print ()\n",
    "    print ('Best route distance')\n",
    "    print (f'{env.state.calculate_distance(best_route):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best average. Batch :     1,  Time    0  Average reward  -6325\n",
      "Batch   1. Average and best run:  -6325,  -1212. \n",
      "New best average. Batch :     2,  Time    0  Average reward  -5276\n",
      "Batch   2. Average and best run:  -5276,   -219. \n",
      "New best average. Batch :     3,  Time    0  Average reward  -4687\n",
      "Batch   5. Average and best run:  -5111,   -615. \n",
      "New best average. Batch :     6,  Time    0  Average reward  -4572\n",
      "Batch   9. Average and best run:  -4657,   -128. \n",
      "New best average. Batch :    10,  Time    0  Average reward  -4562\n",
      "Batch  12. Average and best run:  -4759,     -8. \n",
      "New best average. Batch :    13,  Time    0  Average reward  -4397\n",
      "Batch  13. Average and best run:  -4397,   -770. \n",
      "New best average. Batch :    14,  Time    0  Average reward  -4378\n",
      "Batch  19. Average and best run:  -4597,   -625. \n",
      "New best average. Batch :    20,  Time    0  Average reward  -4366\n",
      "Batch  20. Average and best run:  -4366,   -285. \n",
      "New best average. Batch :    21,  Time    0  Average reward  -3796\n",
      "Batch  44. Average and best run:  -4352,   -419. \n",
      "New best average. Batch :    45,  Time    1  Average reward  -3671\n",
      "Batch  46. Average and best run:  -4388,   -862. \n",
      "New best average. Batch :    47,  Time    1  Average reward  -3453\n",
      "Batch  56. Average and best run:  -3477,    107. \n",
      "New best average. Batch :    57,  Time    1  Average reward  -3339\n",
      "Batch  74. Average and best run:  -3701,      4. \n",
      "New best average. Batch :    75,  Time    1  Average reward  -3014\n",
      "Batch  79. Average and best run:  -3218,    -86. \n",
      "New best average. Batch :    80,  Time    1  Average reward  -2853\n",
      "Batch  91. Average and best run:  -3030,    431. \n",
      "New best average. Batch :    92,  Time    1  Average reward  -2757\n",
      "Batch  94. Average and best run:  -3260,    547. \n",
      "New best average. Batch :    95,  Time    1  Average reward  -2507\n",
      "Batch  97. Average and best run:  -2920,    362. \n",
      "New best average. Batch :    98,  Time    1  Average reward  -2366\n",
      "Batch 107. Average and best run:  -2783,   -694. \n",
      "New best average. Batch :   108,  Time    1  Average reward  -2255\n",
      "Batch 124. Average and best run:  -2320,     35. \n",
      "New best average. Batch :   125,  Time    2  Average reward  -2086\n",
      "Batch 126. Average and best run:  -2184,    208. \n",
      "New best average. Batch :   127,  Time    2  Average reward  -2049\n",
      "Batch 128. Average and best run:  -2119,    340. \n",
      "New best average. Batch :   129,  Time    2  Average reward  -1990\n",
      "Batch 131. Average and best run:  -2271,      4. \n",
      "New best average. Batch :   132,  Time    2  Average reward  -1803\n",
      "Batch 136. Average and best run:  -2316,    649. \n",
      "New best average. Batch :   137,  Time    2  Average reward  -1537\n",
      "Batch 146. Average and best run:  -1647,    566. \n",
      "New best average. Batch :   147,  Time    2  Average reward  -1240\n",
      "Batch 149. Average and best run:  -1262,    606. \n",
      "New best average. Batch :   150,  Time    2  Average reward  -1110\n",
      "Batch 167. Average and best run:  -1154,    690. \n",
      "New best average. Batch :   168,  Time    2  Average reward   -815\n",
      "Batch 169. Average and best run:  -1078,    445. \n",
      "New best average. Batch :   170,  Time    2  Average reward   -665\n",
      "Batch 192. Average and best run:   -683,    647. \n",
      "New best average. Batch :   193,  Time    2  Average reward   -501\n",
      "Batch 193. Average and best run:   -501,    725. \n",
      "New best average. Batch :   194,  Time    2  Average reward   -345\n",
      "Batch 206. Average and best run:   -478,    705. \n",
      "New best average. Batch :   207,  Time    2  Average reward   -327\n",
      "Batch 207. Average and best run:   -327,    701. \n",
      "New best average. Batch :   208,  Time    2  Average reward   -225\n",
      "Batch 208. Average and best run:   -225,    803. \n",
      "New best average. Batch :   209,  Time    2  Average reward     45\n",
      "Batch 216. Average and best run:    -66,    879. \n",
      "New best average. Batch :   217,  Time    3  Average reward    126\n",
      "Batch 224. Average and best run:     92,    900. \n",
      "New best average. Batch :   225,  Time    3  Average reward    139\n",
      "Batch 229. Average and best run:     63,    890. \n",
      "New best average. Batch :   230,  Time    3  Average reward    146\n",
      "Batch 230. Average and best run:    146,    951. \n",
      "New best average. Batch :   231,  Time    3  Average reward    175\n",
      "Batch 231. Average and best run:    175,    876. \n",
      "New best average. Batch :   232,  Time    3  Average reward    224\n",
      "Batch 233. Average and best run:    144,    891. \n",
      "New best average. Batch :   234,  Time    3  Average reward    233\n",
      "Batch 235. Average and best run:    185,    925. \n",
      "New best average. Batch :   236,  Time    3  Average reward    247\n",
      "Batch 236. Average and best run:    247,    888. \n",
      "New best average. Batch :   237,  Time    3  Average reward    278\n",
      "Batch 243. Average and best run:     34,    902. \n",
      "New best average. Batch :   244,  Time    3  Average reward    355\n",
      "Batch 244. Average and best run:    355,    987. \n",
      "New best average. Batch :   245,  Time    3  Average reward    369\n",
      "Batch 246. Average and best run:    261,    942. \n",
      "New best average. Batch :   247,  Time    3  Average reward    552\n",
      "Batch 251. Average and best run:    411,    981. \n",
      "New best average. Batch :   252,  Time    3  Average reward    565\n",
      "Batch 253. Average and best run:    332,    967. \n",
      "New best average. Batch :   254,  Time    3  Average reward    646\n",
      "Batch 255. Average and best run:    236,    998. \n",
      "New best average. Batch :   256,  Time    3  Average reward    649\n",
      "Batch 261. Average and best run:    340,   1013. \n",
      "New best average. Batch :   262,  Time    3  Average reward    652\n",
      "Batch 263. Average and best run:    647,    973. \n",
      "New best average. Batch :   264,  Time    3  Average reward    819\n",
      "Batch 264. Average and best run:    819,    973. "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-be15c5316b88>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Get best runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtraining_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_best_route\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mfilter_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtraining_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtraining_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ff21c40088b1>\u001b[0m in \u001b[0;36mfilter_episodes\u001b[0;34m(results, percentile_cutoff)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Convert list of observation arrays into a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Flatten actions list and convert to NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
